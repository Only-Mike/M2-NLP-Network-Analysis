{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPeMFRJ7pB/UczQ7Mpa8kNr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Only-Mike/M2-NLP-Network-Analysis/blob/main/NLP_SC_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 --- Topic modelling\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "cnPH49yjhgeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing preprocessor to celan our text\n",
        "!pip install tweet-preprocessor -q\n",
        "\n",
        "# Installing Gensim and PyLDAvis\n",
        "!pip install -qq -U gensim\n",
        "!pip install -qq pyLDAvis\n",
        "\n",
        "# explainability (why did the model say it's related to this author)\n",
        "!pip install eli5"
      ],
      "metadata": {
        "id": "qKjsPBmk0MLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tqdm #progress bar\n",
        "import preprocessor as prepro # text prepro\n",
        "\n",
        "import spacy #spacy for quick language prepro\n",
        "nlp = spacy.load('en_core_web_sm') #instantiating English module\n",
        "\n",
        "# sampling, splitting\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# loading ML libraries\n",
        "from sklearn.pipeline import make_pipeline #pipeline creation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer #transforms text to sparse matrix\n",
        "from sklearn.linear_model import LogisticRegression #Logit model\n",
        "from sklearn.metrics import classification_report #that's self explanatory\n",
        "from sklearn.decomposition import TruncatedSVD #dimensionality reduction\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import altair as alt #viz\n",
        "\n",
        "#explainability\n",
        "import eli5\n",
        "from eli5.lime import TextExplainer\n",
        "\n",
        "# topic modeling\n",
        "\n",
        "from gensim.corpora.dictionary import Dictionary # Import the dictionary builder\n",
        "from gensim.models import LdaMulticore # we'll use the faster multicore version of LDA\n",
        "\n",
        "# Import pyLDAvis\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "\n",
        "%matplotlib inline\n",
        "pyLDAvis.enable_notebook()"
      ],
      "metadata": {
        "id": "4s3sclEZ0XQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepro settings\n",
        "prepro.set_options(prepro.OPT.URL, prepro.OPT.NUMBER, prepro.OPT.RESERVED, prepro.OPT.MENTION, prepro.OPT.SMILEY)"
      ],
      "metadata": {
        "id": "yvtO67l74O6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Only-Mike/M2-NLP-Network-Analysis.git"
      ],
      "metadata": {
        "id": "W4D7cL_xzGeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing dataset"
      ],
      "metadata": {
        "id": "iK7-KfnIt_Vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = pd.read_csv('/content/M2-NLP-Network-Analysis/Supply chain network - 2017 csv file.csv')\n",
        "data2 = pd.read_csv('/content/M2-NLP-Network-Analysis/Supply chain network - 2018 csv file.csv')\n",
        "data3 = pd.read_csv('/content/M2-NLP-Network-Analysis/Supply chain network - 2019 csv file.csv')\n",
        "data4 = pd.read_csv('/content/M2-NLP-Network-Analysis/Supply chain network - 2020 csv file.csv')\n",
        "data5 = pd.read_csv('/content/M2-NLP-Network-Analysis/Supply chain network - 2021 csv file.csv')\n"
      ],
      "metadata": {
        "id": "VE52HcDrmRM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames = [data1, data2, data3, data4, data5] #creating frame for all datasets"
      ],
      "metadata": {
        "id": "SWqan3sRmqph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat(frames) #Concat all datasets to \"df\""
      ],
      "metadata": {
        "id": "DqVZQhBOoR4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "e2G109lcpxNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "02u7VJl4p1eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning text and embedding the data"
      ],
      "metadata": {
        "id": "EIBtbZMgt02n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Choosing the columns we need to work with and storing them in the value \"data\". This is for saving the \"df\" dataframe for the prediction model later on.\n",
        "data = df[['Authors', 'Author(s) ID','Title', 'Abstract','Year', 'Source title']]"
      ],
      "metadata": {
        "id": "wbJtLwXO0FqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Take a random sample of 1500 papers. This i for making the model run faster.\n",
        "df.sample(n=1500)"
      ],
      "metadata": {
        "id": "3D0m7Qaxe6Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Making a new column called \"text\" from the abstract column\n",
        "data['text'] = data['Abstract']"
      ],
      "metadata": {
        "id": "3aB3k2CWi8Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUhjywn2gBz0"
      },
      "source": [
        "#Cleaning the text\n",
        "data['text_clean'] = data['text'].map(lambda t: prepro.clean(t))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run progress bar and clean up using spacy but without some heavy parts of the pipeline\n",
        "\n",
        "clean_text = []\n",
        "\n",
        "pbar = tqdm.tqdm(total=len(data['text_clean']),position=0, leave=True)\n",
        "\n",
        "for text in nlp.pipe(data['text_clean'], disable=[\"tagger\", \"parser\", \"ner\"]):\n",
        "\n",
        "  txt = [token.lemma_.lower() for token in text \n",
        "         if token.is_alpha \n",
        "         and not token.is_stop \n",
        "         and not token.is_punct]\n",
        "\n",
        "  clean_text.append(\" \".join(txt))\n",
        "\n",
        "  pbar.update(1)"
      ],
      "metadata": {
        "id": "7RFrxl4r4ADK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write everything into a single function for simplicity later on\n",
        "def text_prepro(texts):\n",
        "  \"\"\"\n",
        "  takes in a pandas series (1 column of a DF)\n",
        "  lowercases, normalizes text\n",
        "  \"\"\"\n",
        "  texts_clean = texts.map(lambda t: prepro.clean(t))\n",
        "\n",
        "  clean_container = []\n",
        "\n",
        "  pbar = tqdm.tqdm(total=len(texts_clean),position=0, leave=True)\n",
        "\n",
        "  for text in nlp.pipe(texts_clean, disable=[\"tagger\", \"parser\", \"ner\"]):\n",
        "\n",
        "    txt = [token.lemma_.lower() for token in text \n",
        "          if token.is_alpha \n",
        "          and not token.is_stop \n",
        "          and not token.is_punct]\n",
        "\n",
        "    clean_container.append(\" \".join(txt))\n",
        "    pbar.update(1)\n",
        "  \n",
        "  return clean_container"
      ],
      "metadata": {
        "id": "FD-e1YEV4B-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['text_clean'] = text_prepro(data['text'])  ##<---- HVAD GØR DEN HER?<-------##"
      ],
      "metadata": {
        "id": "gQOci1Od5cyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess texts (we need tokens)  <---- Hvad gør den her? <------\n",
        "tokens = []\n",
        "\n",
        "for summary in nlp.pipe(data['text_clean'], disable=[\"ner\"]):\n",
        "  proj_tok = [token.lemma_.lower() for token in summary \n",
        "              if token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV'] \n",
        "              and not token.is_stop\n",
        "              and not token.is_punct] \n",
        "  tokens.append(proj_tok)"
      ],
      "metadata": {
        "id": "JkXzAz6S5lpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['tokens'] = tokens"
      ],
      "metadata": {
        "id": "iKEJR8GU6m3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## SPØRGSMÅL OMKRING FILTER \n",
        "\n",
        "# Create a Dictionary from the articles: dictionary\n",
        "dictionary = Dictionary(data['tokens'])\n",
        "\n",
        "# filter out low-frequency / high-frequency stuff, also limit the vocabulary to max XXXXXXX words\n",
        "dictionary.filter_extremes(no_below=4, no_above=0.4, keep_n=600)\n",
        "\n",
        "# construct corpus using this dictionary\n",
        "corpus = [dictionary.doc2bow(doc) for doc in data['tokens']]"
      ],
      "metadata": {
        "id": "-5qooRNP7GdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visualization"
      ],
      "metadata": {
        "id": "FVQFsJTk_4iJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model = LdaMulticore(corpus, id2word=dictionary, num_topics=13, workers = 4, passes=10)"
      ],
      "metadata": {
        "id": "7ghvRrTM7SRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)"
      ],
      "metadata": {
        "id": "ohqvwK17-cvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pyLDAvis.display(lda_display)"
      ],
      "metadata": {
        "id": "_4EfZAei-fXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ete6u0qu-hao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Metrics"
      ],
      "metadata": {
        "id": "SrYbiev7__us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from gensim.models import LdaModel, CoherenceModel\n",
        "from gensim import corpora"
      ],
      "metadata": {
        "id": "gB_lko8FsN8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = data['tokens']"
      ],
      "metadata": {
        "id": "YC4LGwpOsXCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "id": "a4hWPw6xxW6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dirichlet_dict = corpora.Dictionary(corpus)\n",
        "bow_corpus = [dirichlet_dict.doc2bow(text) for text in corpus]\n",
        "\n",
        "# Considering 1-15 topics, as the last is cut off\n",
        "num_topics = list(range(16)[1:])\n",
        "num_keywords = 15\n",
        "\n",
        "LDA_models = {}\n",
        "LDA_topics = {}\n",
        "for i in num_topics:\n",
        "    LDA_models[i] = LdaModel(corpus=bow_corpus,\n",
        "                             id2word=dirichlet_dict,\n",
        "                             num_topics=i,\n",
        "                             update_every=1,\n",
        "                             chunksize=len(bow_corpus),\n",
        "                             passes=20,\n",
        "                             alpha='auto',\n",
        "                             random_state=42)\n",
        "\n",
        "    shown_topics = LDA_models[i].show_topics(num_topics=i, \n",
        "                                             num_words=num_keywords,\n",
        "                                             formatted=False)\n",
        "    LDA_topics[i] = [[word[0] for word in topic[1]] for topic in shown_topics]"
      ],
      "metadata": {
        "id": "g6M0SE0nr7KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(topic_1, topic_2):\n",
        "    \"\"\"\n",
        "    Derives the Jaccard similarity of two topics\n",
        "\n",
        "    Jaccard similarity:\n",
        "    - A statistic used for comparing the similarity and diversity of sample sets\n",
        "    - J(A,B) = (A ∩ B)/(A ∪ B)\n",
        "    - Goal is low Jaccard scores for coverage of the diverse elements\n",
        "    \"\"\"\n",
        "    intersection = set(topic_1).intersection(set(topic_2))\n",
        "    union = set(topic_1).union(set(topic_2))\n",
        "                    \n",
        "    return float(len(intersection))/float(len(union))"
      ],
      "metadata": {
        "id": "ZzK6Q8B1s5gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LDA_stability = {}\n",
        "for i in range(0, len(num_topics)-1):\n",
        "    jaccard_sims = []\n",
        "    for t1, topic1 in enumerate(LDA_topics[num_topics[i]]): # pylint: disable=unused-variable\n",
        "        sims = []\n",
        "        for t2, topic2 in enumerate(LDA_topics[num_topics[i+1]]): # pylint: disable=unused-variable\n",
        "            sims.append(jaccard_similarity(topic1, topic2))    \n",
        "        \n",
        "        jaccard_sims.append(sims)    \n",
        "    \n",
        "    LDA_stability[num_topics[i]] = jaccard_sims\n",
        "                \n",
        "mean_stabilities = [np.array(LDA_stability[i]).mean() for i in num_topics[:-1]]"
      ],
      "metadata": {
        "id": "8CqBHt5ewRjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coherences = [CoherenceModel(model=LDA_models[i], texts=corpus, dictionary=dirichlet_dict, coherence='c_v').get_coherence() for i in num_topics[:-1]]"
      ],
      "metadata": {
        "id": "LxgSm8mbwWSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coh_sta_diffs = [coherences[i] - mean_stabilities[i] for i in range(num_keywords)[:-1]] # limit topic numbers to the number of keywords\n",
        "coh_sta_max = max(coh_sta_diffs)\n",
        "coh_sta_max_idxs = [i for i, j in enumerate(coh_sta_diffs) if j == coh_sta_max]\n",
        "ideal_topic_num_index = coh_sta_max_idxs[0] # choose less topics in case there's more than one max\n",
        "ideal_topic_num = num_topics[ideal_topic_num_index]"
      ],
      "metadata": {
        "id": "FgQ7NBBOwzru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "ax = sns.lineplot(x=num_topics[:-1], y=mean_stabilities, label='Average Topic Overlap')\n",
        "ax = sns.lineplot(x=num_topics[:-1], y=coherences, label='Topic Coherence')\n",
        "\n",
        "ax.axvline(x=ideal_topic_num, label='Ideal Number of Topics', color='black')\n",
        "ax.axvspan(xmin=ideal_topic_num - 1, xmax=ideal_topic_num + 1, alpha=0.5, facecolor='grey')\n",
        "\n",
        "y_max = max(max(mean_stabilities), max(coherences)) + (0.10 * max(max(mean_stabilities), max(coherences)))\n",
        "ax.set_ylim([0, y_max])\n",
        "ax.set_xlim([1, num_topics[-1]-1])\n",
        "                \n",
        "ax.axes.set_title('Model Metrics per Number of Topics', fontsize=25)\n",
        "ax.set_ylabel('Metric Level', fontsize=20)\n",
        "ax.set_xlabel('Number of Topics', fontsize=20)\n",
        "plt.legend(fontsize=20)\n",
        "plt.show()  "
      ],
      "metadata": {
        "id": "69zOLWwGwb_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 --- Label Prediction\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YgpTp3BdhysG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning"
      ],
      "metadata": {
        "id": "0pa1G0C_kFFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "shEuRWDsiAdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yKPVjt-8iZ7f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}