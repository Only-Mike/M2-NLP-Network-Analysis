{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP3tYhV5ju6NbaAm6wI+d1j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Only-Mike/M2-NLP-Network-Analysis/blob/main/NLP_SC_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook for network analysis and Natural Language proccesing.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This note book has been created by Kasper R. Haurum, Mike Christensen, Rayian Alam and Snorre K. Brouer."
      ],
      "metadata": {
        "id": "ouemGEbyNV9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import dataset as df\n",
        "\n",
        "This NEEDS to be run first before going further into the assignment"
      ],
      "metadata": {
        "id": "BYQee65LE6Q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "OcTXcZl7EwKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing dataset"
      ],
      "metadata": {
        "id": "iK7-KfnIt_Vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Only-Mike/M2-NLP-Network-Analysis.git"
      ],
      "metadata": {
        "id": "P2XBqMexK_Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = pd.read_csv('/content/M2-NLP-Network-Analysis/Supply chain network - 2017 csv file.csv')\n",
        "data2 = pd.read_csv('/content/M2-NLP-Network-Analysis/Supply chain network - 2018 csv file.csv')\n",
        "data3 = pd.read_csv('/content/M2-NLP-Network-Analysis/Supply chain network - 2019 csv file.csv')\n",
        "data4 = pd.read_csv('/content/M2-NLP-Network-Analysis/Supply chain network - 2020 csv file.csv')\n",
        "data5 = pd.read_csv('/content/M2-NLP-Network-Analysis/Supply chain network - 2021 csv file.csv')\n"
      ],
      "metadata": {
        "id": "VE52HcDrmRM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames = [data1, data2, data3, data4, data5] #creating frame for all datasets"
      ],
      "metadata": {
        "id": "SWqan3sRmqph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat(frames) #Concat all datasets to \"df\""
      ],
      "metadata": {
        "id": "DqVZQhBOoR4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Topic modelling\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "cnPH49yjhgeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing preprocessor to celan our text\n",
        "!pip install tweet-preprocessor -q\n",
        "\n",
        "# Installing Gensim and PyLDAvis\n",
        "!pip install -qq -U gensim\n",
        "!pip install -qq pyLDAvis\n",
        "\n",
        "# explainability (why did the model say it's related to this author)\n",
        "!pip install eli5"
      ],
      "metadata": {
        "id": "qKjsPBmk0MLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tqdm #progress bar\n",
        "import preprocessor as prepro # text prepro\n",
        "\n",
        "import spacy #spacy for quick language prepro\n",
        "nlp = spacy.load('en_core_web_sm') #instantiating English module\n",
        "\n",
        "# sampling, splitting\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# loading ML libraries\n",
        "from sklearn.pipeline import make_pipeline #pipeline creation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer #transforms text to sparse matrix\n",
        "from sklearn.linear_model import LogisticRegression #Logit model\n",
        "from sklearn.metrics import classification_report #that's self explanatory\n",
        "from sklearn.decomposition import TruncatedSVD #dimensionality reduction\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import altair as alt #viz\n",
        "\n",
        "#explainability\n",
        "import eli5\n",
        "from eli5.lime import TextExplainer\n",
        "\n",
        "# topic modeling\n",
        "\n",
        "from gensim.corpora.dictionary import Dictionary # Import the dictionary builder\n",
        "from gensim.models import LdaMulticore # we'll use the faster multicore version of LDA\n",
        "\n",
        "# Import pyLDAvis\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "\n",
        "%matplotlib inline\n",
        "pyLDAvis.enable_notebook()"
      ],
      "metadata": {
        "id": "4s3sclEZ0XQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepro settings\n",
        "prepro.set_options(prepro.OPT.URL, prepro.OPT.NUMBER, prepro.OPT.RESERVED, prepro.OPT.MENTION, prepro.OPT.SMILEY)"
      ],
      "metadata": {
        "id": "yvtO67l74O6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "02u7VJl4p1eX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning text and embedding the data"
      ],
      "metadata": {
        "id": "EIBtbZMgt02n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Choosing the columns we need to work with and storing them in the value \"data\". This is for saving the \"df\" dataframe for the prediction model later on.\n",
        "data = df[['Authors', 'Author(s) ID','Title', 'Abstract','Year', 'Source title']]"
      ],
      "metadata": {
        "id": "wbJtLwXO0FqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Take a random sample of 1500 papers. This i for making the model run faster.\n",
        "data = data.sample(n=1500)"
      ],
      "metadata": {
        "id": "3D0m7Qaxe6Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Making a new column called \"text\" from the abstract column\n",
        "data['text'] = data['Abstract']"
      ],
      "metadata": {
        "id": "3aB3k2CWi8Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUhjywn2gBz0"
      },
      "source": [
        "#Cleaning the text\n",
        "data['text_clean'] = data['text'].map(lambda t: prepro.clean(t))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run progress bar and clean up using spacy but without some heavy parts of the pipeline\n",
        "\n",
        "clean_text = []\n",
        "\n",
        "pbar = tqdm.tqdm(total=len(data['text_clean']),position=0, leave=True)\n",
        "\n",
        "for text in nlp.pipe(data['text_clean'], disable=[\"tagger\", \"parser\", \"ner\"]):\n",
        "\n",
        "  txt = [token.lemma_.lower() for token in text \n",
        "         if token.is_alpha \n",
        "         and not token.is_stop \n",
        "         and not token.is_punct]\n",
        "\n",
        "  clean_text.append(\" \".join(txt))\n",
        "\n",
        "  pbar.update(1)"
      ],
      "metadata": {
        "id": "7RFrxl4r4ADK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write everything into a single function for simplicity later on\n",
        "def text_prepro(texts):\n",
        "  \"\"\"\n",
        "  takes in a pandas series (1 column of a DF)\n",
        "  lowercases, normalizes text\n",
        "  \"\"\"\n",
        "  texts_clean = texts.map(lambda t: prepro.clean(t))\n",
        "\n",
        "  clean_container = []\n",
        "\n",
        "  pbar = tqdm.tqdm(total=len(texts_clean),position=0, leave=True)\n",
        "\n",
        "  for text in nlp.pipe(texts_clean, disable=[\"tagger\", \"parser\", \"ner\"]):\n",
        "\n",
        "    txt = [token.lemma_.lower() for token in text \n",
        "          if token.is_alpha \n",
        "          and not token.is_stop \n",
        "          and not token.is_punct]\n",
        "\n",
        "    clean_container.append(\" \".join(txt))\n",
        "    pbar.update(1)\n",
        "  \n",
        "  return clean_container"
      ],
      "metadata": {
        "id": "FD-e1YEV4B-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['text_clean'] = text_prepro(data['text'])"
      ],
      "metadata": {
        "id": "gQOci1Od5cyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess texts\n",
        "tokens = []\n",
        "\n",
        "for summary in nlp.pipe(data['text_clean'], disable=[\"ner\"]):\n",
        "  proj_tok = [token.lemma_.lower() for token in summary \n",
        "              if token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV'] \n",
        "              and not token.is_stop\n",
        "              and not token.is_punct] \n",
        "  tokens.append(proj_tok)"
      ],
      "metadata": {
        "id": "JkXzAz6S5lpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['tokens'] = tokens"
      ],
      "metadata": {
        "id": "iKEJR8GU6m3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Dictionary from the articles: dictionary\n",
        "dictionary = Dictionary(data['tokens'])\n",
        "\n",
        "# filter out low-frequency / high-frequency stuff, also limit the vocabulary to max 600 words\n",
        "dictionary.filter_extremes(no_below=4, no_above=0.4, keep_n=600)\n",
        "\n",
        "# construct corpus using this dictionary\n",
        "corpus = [dictionary.doc2bow(doc) for doc in data['tokens']]"
      ],
      "metadata": {
        "id": "-5qooRNP7GdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Visualization"
      ],
      "metadata": {
        "id": "FVQFsJTk_4iJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model = LdaMulticore(corpus, id2word=dictionary, num_topics=13, workers = 4, passes=10)"
      ],
      "metadata": {
        "id": "7ghvRrTM7SRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)"
      ],
      "metadata": {
        "id": "ohqvwK17-cvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pyLDAvis.display(lda_display)"
      ],
      "metadata": {
        "id": "_4EfZAei-fXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ete6u0qu-hao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Metrics"
      ],
      "metadata": {
        "id": "SrYbiev7__us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from gensim.models import LdaModel, CoherenceModel\n",
        "from gensim import corpora"
      ],
      "metadata": {
        "id": "gB_lko8FsN8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = data['tokens']"
      ],
      "metadata": {
        "id": "YC4LGwpOsXCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "id": "a4hWPw6xxW6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dirichlet_dict = corpora.Dictionary(corpus)\n",
        "bow_corpus = [dirichlet_dict.doc2bow(text) for text in corpus]\n",
        "\n",
        "# Considering 1-15 topics, as the last is cut off\n",
        "num_topics = list(range(16)[1:])\n",
        "num_keywords = 15\n",
        "\n",
        "LDA_models = {}\n",
        "LDA_topics = {}\n",
        "for i in num_topics:\n",
        "    LDA_models[i] = LdaModel(corpus=bow_corpus,\n",
        "                             id2word=dirichlet_dict,\n",
        "                             num_topics=i,\n",
        "                             update_every=1,\n",
        "                             chunksize=len(bow_corpus),\n",
        "                             passes=20,\n",
        "                             alpha='auto',\n",
        "                             random_state=42)\n",
        "\n",
        "    shown_topics = LDA_models[i].show_topics(num_topics=i, \n",
        "                                             num_words=num_keywords,\n",
        "                                             formatted=False)\n",
        "    LDA_topics[i] = [[word[0] for word in topic[1]] for topic in shown_topics]"
      ],
      "metadata": {
        "id": "g6M0SE0nr7KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(topic_1, topic_2):\n",
        "    \"\"\"\n",
        "    Derives the Jaccard similarity of two topics\n",
        "\n",
        "    Jaccard similarity:\n",
        "    - A statistic used for comparing the similarity and diversity of sample sets\n",
        "    - J(A,B) = (A ∩ B)/(A ∪ B)\n",
        "    - Goal is low Jaccard scores for coverage of the diverse elements\n",
        "    \"\"\"\n",
        "    intersection = set(topic_1).intersection(set(topic_2))\n",
        "    union = set(topic_1).union(set(topic_2))\n",
        "                    \n",
        "    return float(len(intersection))/float(len(union))"
      ],
      "metadata": {
        "id": "ZzK6Q8B1s5gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LDA_stability = {}\n",
        "for i in range(0, len(num_topics)-1):\n",
        "    jaccard_sims = []\n",
        "    for t1, topic1 in enumerate(LDA_topics[num_topics[i]]): # pylint: disable=unused-variable\n",
        "        sims = []\n",
        "        for t2, topic2 in enumerate(LDA_topics[num_topics[i+1]]): # pylint: disable=unused-variable\n",
        "            sims.append(jaccard_similarity(topic1, topic2))    \n",
        "        \n",
        "        jaccard_sims.append(sims)    \n",
        "    \n",
        "    LDA_stability[num_topics[i]] = jaccard_sims\n",
        "                \n",
        "mean_stabilities = [np.array(LDA_stability[i]).mean() for i in num_topics[:-1]]"
      ],
      "metadata": {
        "id": "8CqBHt5ewRjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coherences = [CoherenceModel(model=LDA_models[i], texts=corpus, dictionary=dirichlet_dict, coherence='c_v').get_coherence() for i in num_topics[:-1]]"
      ],
      "metadata": {
        "id": "LxgSm8mbwWSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coh_sta_diffs = [coherences[i] - mean_stabilities[i] for i in range(num_keywords)[:-1]] # limit topic numbers to the number of keywords\n",
        "coh_sta_max = max(coh_sta_diffs)\n",
        "coh_sta_max_idxs = [i for i, j in enumerate(coh_sta_diffs) if j == coh_sta_max]\n",
        "ideal_topic_num_index = coh_sta_max_idxs[0] # choose less topics in case there's more than one max\n",
        "ideal_topic_num = num_topics[ideal_topic_num_index]"
      ],
      "metadata": {
        "id": "FgQ7NBBOwzru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "ax = sns.lineplot(x=num_topics[:-1], y=mean_stabilities, label='Average Topic Overlap')\n",
        "ax = sns.lineplot(x=num_topics[:-1], y=coherences, label='Topic Coherence')\n",
        "\n",
        "ax.axvline(x=ideal_topic_num, label='Ideal Number of Topics', color='black')\n",
        "ax.axvspan(xmin=ideal_topic_num - 1, xmax=ideal_topic_num + 1, alpha=0.5, facecolor='grey')\n",
        "\n",
        "y_max = max(max(mean_stabilities), max(coherences)) + (0.10 * max(max(mean_stabilities), max(coherences)))\n",
        "ax.set_ylim([0, y_max])\n",
        "ax.set_xlim([1, num_topics[-1]-1])\n",
        "                \n",
        "ax.axes.set_title('Model Metrics per Number of Topics', fontsize=25)\n",
        "ax.set_ylabel('Metric Level', fontsize=20)\n",
        "ax.set_xlabel('Number of Topics', fontsize=20)\n",
        "plt.legend(fontsize=20)\n",
        "plt.show()  "
      ],
      "metadata": {
        "id": "69zOLWwGwb_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Topics over time\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "540tgjnS0IX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Topics 2017"
      ],
      "metadata": {
        "id": "-aWkI_xgEDgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Choosing the columns we need to work with and storing them in the value \"data\". This is for saving the \"df\" dataframe for the prediction model later on.\n",
        "data1 = data1[['Authors', 'Author(s) ID','Title', 'Abstract','Year', 'Source title']]"
      ],
      "metadata": {
        "id": "eV4yGh2iGZ0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Take a random sample of 500 papers. This i for making the model run faster.\n",
        "data1 = data1.sample(n=500)"
      ],
      "metadata": {
        "id": "ceMx4XjeGZ0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Making a new column called \"text\" from the abstract column\n",
        "data1['text'] = data1['Abstract']"
      ],
      "metadata": {
        "id": "-qplwKS6GZ0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH4ZsCuUGZ0o"
      },
      "source": [
        "#Cleaning the text\n",
        "data1['text_clean'] = data1['text'].map(lambda t: prepro.clean(t))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run progress bar and clean up using spacy but without some heavy parts of the pipeline\n",
        "\n",
        "clean_text = []\n",
        "\n",
        "pbar = tqdm.tqdm(total=len(data1['text_clean']),position=0, leave=True)\n",
        "\n",
        "for text in nlp.pipe(data1['text_clean'], disable=[\"tagger\", \"parser\", \"ner\"]):\n",
        "\n",
        "  txt = [token.lemma_.lower() for token in text \n",
        "         if token.is_alpha \n",
        "         and not token.is_stop \n",
        "         and not token.is_punct]\n",
        "\n",
        "  clean_text.append(\" \".join(txt))\n",
        "\n",
        "  pbar.update(1)"
      ],
      "metadata": {
        "id": "UGRLL0coGZ0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write everything into a single function for simplicity later on\n",
        "def text_prepro(texts):\n",
        "  \"\"\"\n",
        "  takes in a pandas series (1 column of a DF)\n",
        "  lowercases, normalizes text\n",
        "  \"\"\"\n",
        "  texts_clean = texts.map(lambda t: prepro.clean(t))\n",
        "\n",
        "  clean_container = []\n",
        "\n",
        "  pbar = tqdm.tqdm(total=len(texts_clean),position=0, leave=True)\n",
        "\n",
        "  for text in nlp.pipe(texts_clean, disable=[\"tagger\", \"parser\", \"ner\"]):\n",
        "\n",
        "    txt = [token.lemma_.lower() for token in text \n",
        "          if token.is_alpha \n",
        "          and not token.is_stop \n",
        "          and not token.is_punct]\n",
        "\n",
        "    clean_container.append(\" \".join(txt))\n",
        "    pbar.update(1)\n",
        "  \n",
        "  return clean_container"
      ],
      "metadata": {
        "id": "d6gaizgoGZ0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1['text_clean'] = text_prepro(data1['text'])  ##<---- HVAD GØR DEN HER?<-------##"
      ],
      "metadata": {
        "id": "aVd3P0Z1GZ0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess texts (we need tokens)  <---- Hvad gør den her? <------\n",
        "tokens = []\n",
        "\n",
        "for summary in nlp.pipe(data1['text_clean'], disable=[\"ner\"]):\n",
        "  proj_tok = [token.lemma_.lower() for token in summary \n",
        "              if token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV'] \n",
        "              and not token.is_stop\n",
        "              and not token.is_punct] \n",
        "  tokens.append(proj_tok)"
      ],
      "metadata": {
        "id": "SUO-8yK5GZ0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1['tokens'] = tokens"
      ],
      "metadata": {
        "id": "MLv7lIMpGZ0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a Dictionary from the articles: dictionary\n",
        "dictionary = Dictionary(data1['tokens'])\n",
        "\n",
        "# filter out low-frequency / high-frequency stuff, also limit the vocabulary to max XXXXXXX words\n",
        "dictionary.filter_extremes(no_below=4, no_above=0.4, keep_n=600)\n",
        "\n",
        "# construct corpus using this dictionary\n",
        "corpus = [dictionary.doc2bow(doc) for doc in data1['tokens']]"
      ],
      "metadata": {
        "id": "4EgM915HGZ0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Visualization"
      ],
      "metadata": {
        "id": "N_jDvDYsGZ0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model = LdaMulticore(corpus, id2word=dictionary, num_topics=13, workers = 4, passes=10)"
      ],
      "metadata": {
        "id": "tsKmgHC0GZ0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)"
      ],
      "metadata": {
        "id": "hUDX6nhyGZ0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pyLDAvis.display(lda_display)"
      ],
      "metadata": {
        "id": "nK1yI-X6GZ0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topics 2018"
      ],
      "metadata": {
        "id": "2xUauvPIEI_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Choosing the columns we need to work with and storing them in the value \"data\". This is for saving the \"df\" dataframe for the prediction model later on.\n",
        "data2 = data2[['Authors', 'Author(s) ID','Title', 'Abstract','Year', 'Source title']]"
      ],
      "metadata": {
        "id": "rNSZoZn4HJPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Take a random sample of 500 papers. This i for making the model run faster.\n",
        "data2 = data2.sample(n=500)"
      ],
      "metadata": {
        "id": "fdAl7_h4HJPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Making a new column called \"text\" from the abstract column\n",
        "data2['text'] = data2['Abstract']"
      ],
      "metadata": {
        "id": "dgLGZDeqHJPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv_S8tBIHJPm"
      },
      "source": [
        "#Cleaning the text\n",
        "data2['text_clean'] = data2['text'].map(lambda t: prepro.clean(t))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run progress bar and clean up using spacy but without some heavy parts of the pipeline\n",
        "\n",
        "clean_text = []\n",
        "\n",
        "pbar = tqdm.tqdm(total=len(data2['text_clean']),position=0, leave=True)\n",
        "\n",
        "for text in nlp.pipe(data2['text_clean'], disable=[\"tagger\", \"parser\", \"ner\"]):\n",
        "\n",
        "  txt = [token.lemma_.lower() for token in text \n",
        "         if token.is_alpha \n",
        "         and not token.is_stop \n",
        "         and not token.is_punct]\n",
        "\n",
        "  clean_text.append(\" \".join(txt))\n",
        "\n",
        "  pbar.update(1)"
      ],
      "metadata": {
        "id": "EgKjQ_1jHJPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write everything into a single function for simplicity later on\n",
        "def text_prepro(texts):\n",
        "  \"\"\"\n",
        "  takes in a pandas series (1 column of a DF)\n",
        "  lowercases, normalizes text\n",
        "  \"\"\"\n",
        "  texts_clean = texts.map(lambda t: prepro.clean(t))\n",
        "\n",
        "  clean_container = []\n",
        "\n",
        "  pbar = tqdm.tqdm(total=len(texts_clean),position=0, leave=True)\n",
        "\n",
        "  for text in nlp.pipe(texts_clean, disable=[\"tagger\", \"parser\", \"ner\"]):\n",
        "\n",
        "    txt = [token.lemma_.lower() for token in text \n",
        "          if token.is_alpha \n",
        "          and not token.is_stop \n",
        "          and not token.is_punct]\n",
        "\n",
        "    clean_container.append(\" \".join(txt))\n",
        "    pbar.update(1)\n",
        "  \n",
        "  return clean_container"
      ],
      "metadata": {
        "id": "276tImuQHJPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2['text_clean'] = text_prepro(data2['text'])"
      ],
      "metadata": {
        "id": "N2W3UL2AHJPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess texts\n",
        "tokens = []\n",
        "\n",
        "for summary in nlp.pipe(data2['text_clean'], disable=[\"ner\"]):\n",
        "  proj_tok = [token.lemma_.lower() for token in summary \n",
        "              if token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV'] \n",
        "              and not token.is_stop\n",
        "              and not token.is_punct] \n",
        "  tokens.append(proj_tok)"
      ],
      "metadata": {
        "id": "Is2MjtDRHJPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2['tokens'] = tokens"
      ],
      "metadata": {
        "id": "yHef6fNvHJPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Dictionary from the articles: dictionary\n",
        "dictionary = Dictionary(data2['tokens'])\n",
        "\n",
        "# filter out low-frequency / high-frequency stuff, also limit the vocabulary to max 600 words\n",
        "dictionary.filter_extremes(no_below=4, no_above=0.4, keep_n=600)\n",
        "\n",
        "# construct corpus using this dictionary\n",
        "corpus = [dictionary.doc2bow(doc) for doc in data2['tokens']]"
      ],
      "metadata": {
        "id": "PVpSpwYYHJPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Visualization"
      ],
      "metadata": {
        "id": "Wzll_ZATHJPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model = LdaMulticore(corpus, id2word=dictionary, num_topics=13, workers = 4, passes=10)"
      ],
      "metadata": {
        "id": "X-oVY6E-HJPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)"
      ],
      "metadata": {
        "id": "ZEVDwWd8HJPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pyLDAvis.display(lda_display)"
      ],
      "metadata": {
        "id": "IA3z5pFdHJPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bTmFNNB9HJPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Topics 2019"
      ],
      "metadata": {
        "id": "Q9ZRp5zFEMqq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XZPs4hY_EPPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Choosing the columns we need to work with and storing them in the value \"data\". This is for saving the \"df\" dataframe for the prediction model later on.\n",
        "data3 = data3[['Authors', 'Author(s) ID','Title', 'Abstract','Year', 'Source title']]"
      ],
      "metadata": {
        "id": "Iyxi_sPAH-4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Take a random sample of 500 papers. This i for making the model run faster.\n",
        "data3 = data3.sample(n=500)"
      ],
      "metadata": {
        "id": "7LFlBzLTH-4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Making a new column called \"text\" from the abstract column\n",
        "data3['text'] = data3['Abstract']"
      ],
      "metadata": {
        "id": "4X9y4r6RH-4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWSDguoIH-4U"
      },
      "source": [
        "#Cleaning the text\n",
        "data3['text_clean'] = data3['text'].map(lambda t: prepro.clean(t))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run progress bar and clean up using spacy but without some heavy parts of the pipeline\n",
        "\n",
        "clean_text = []\n",
        "\n",
        "pbar = tqdm.tqdm(total=len(data3['text_clean']),position=0, leave=True)\n",
        "\n",
        "for text in nlp.pipe(data3['text_clean'], disable=[\"tagger\", \"parser\", \"ner\"]):\n",
        "\n",
        "  txt = [token.lemma_.lower() for token in text \n",
        "         if token.is_alpha \n",
        "         and not token.is_stop \n",
        "         and not token.is_punct]\n",
        "\n",
        "  clean_text.append(\" \".join(txt))\n",
        "\n",
        "  pbar.update(1)"
      ],
      "metadata": {
        "id": "ILkJKqwXH-4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write everything into a single function for simplicity later on\n",
        "def text_prepro(texts):\n",
        "  \"\"\"\n",
        "  takes in a pandas series (1 column of a DF)\n",
        "  lowercases, normalizes text\n",
        "  \"\"\"\n",
        "  texts_clean = texts.map(lambda t: prepro.clean(t))\n",
        "\n",
        "  clean_container = []\n",
        "\n",
        "  pbar = tqdm.tqdm(total=len(texts_clean),position=0, leave=True)\n",
        "\n",
        "  for text in nlp.pipe(texts_clean, disable=[\"tagger\", \"parser\", \"ner\"]):\n",
        "\n",
        "    txt = [token.lemma_.lower() for token in text \n",
        "          if token.is_alpha \n",
        "          and not token.is_stop \n",
        "          and not token.is_punct]\n",
        "\n",
        "    clean_container.append(\" \".join(txt))\n",
        "    pbar.update(1)\n",
        "  \n",
        "  return clean_container"
      ],
      "metadata": {
        "id": "YJ_-dBECH-4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data3['text_clean'] = text_prepro(data3['text'])"
      ],
      "metadata": {
        "id": "AHp0yQb3H-4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess texts\n",
        "tokens = []\n",
        "\n",
        "for summary in nlp.pipe(data3['text_clean'], disable=[\"ner\"]):\n",
        "  proj_tok = [token.lemma_.lower() for token in summary \n",
        "              if token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV'] \n",
        "              and not token.is_stop\n",
        "              and not token.is_punct] \n",
        "  tokens.append(proj_tok)"
      ],
      "metadata": {
        "id": "kq5XCh8KH-4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data3['tokens'] = tokens"
      ],
      "metadata": {
        "id": "5E2tBGU0H-4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Dictionary from the articles: dictionary\n",
        "dictionary = Dictionary(data3['tokens'])\n",
        "\n",
        "# filter out low-frequency / high-frequency stuff, also limit the vocabulary to max 600 words\n",
        "dictionary.filter_extremes(no_below=4, no_above=0.4, keep_n=600)\n",
        "\n",
        "# construct corpus using this dictionary\n",
        "corpus = [dictionary.doc2bow(doc) for doc in data3['tokens']]"
      ],
      "metadata": {
        "id": "3faIU95rH-4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Visualization"
      ],
      "metadata": {
        "id": "JQYR_1GUH-4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model = LdaMulticore(corpus, id2word=dictionary, num_topics=13, workers = 4, passes=10)"
      ],
      "metadata": {
        "id": "H_8IzBAhH-4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)"
      ],
      "metadata": {
        "id": "Yox-hw9JH-4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pyLDAvis.display(lda_display)"
      ],
      "metadata": {
        "id": "prLP0QMYH-4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Topics 2020"
      ],
      "metadata": {
        "id": "sYPxESicEPnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Choosing the columns we need to work with and storing them in the value \"data\". This is for saving the \"df\" dataframe for the prediction model later on.\n",
        "data4 = data4[['Authors', 'Author(s) ID','Title', 'Abstract','Year', 'Source title']]"
      ],
      "metadata": {
        "id": "RkKP1A4uH_xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Take a random sample of 500 papers. This i for making the model run faster.\n",
        "data4 = data4.sample(n=500)"
      ],
      "metadata": {
        "id": "H699t9CBH_xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Making a new column called \"text\" from the abstract column\n",
        "data4['text'] = data4['Abstract']"
      ],
      "metadata": {
        "id": "5loFE1tBH_xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dgvg9_2PH_xV"
      },
      "source": [
        "#Cleaning the text\n",
        "data4['text_clean'] = data4['text'].map(lambda t: prepro.clean(t))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run progress bar and clean up using spacy but without some heavy parts of the pipeline\n",
        "\n",
        "clean_text = []\n",
        "\n",
        "pbar = tqdm.tqdm(total=len(data4['text_clean']),position=0, leave=True)\n",
        "\n",
        "for text in nlp.pipe(data4['text_clean'], disable=[\"tagger\", \"parser\", \"ner\"]):\n",
        "\n",
        "  txt = [token.lemma_.lower() for token in text \n",
        "         if token.is_alpha \n",
        "         and not token.is_stop \n",
        "         and not token.is_punct]\n",
        "\n",
        "  clean_text.append(\" \".join(txt))\n",
        "\n",
        "  pbar.update(1)"
      ],
      "metadata": {
        "id": "ao6x896SH_xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write everything into a single function for simplicity later on\n",
        "def text_prepro(texts):\n",
        "  \"\"\"\n",
        "  takes in a pandas series (1 column of a DF)\n",
        "  lowercases, normalizes text\n",
        "  \"\"\"\n",
        "  texts_clean = texts.map(lambda t: prepro.clean(t))\n",
        "\n",
        "  clean_container = []\n",
        "\n",
        "  pbar = tqdm.tqdm(total=len(texts_clean),position=0, leave=True)\n",
        "\n",
        "  for text in nlp.pipe(texts_clean, disable=[\"tagger\", \"parser\", \"ner\"]):\n",
        "\n",
        "    txt = [token.lemma_.lower() for token in text \n",
        "          if token.is_alpha \n",
        "          and not token.is_stop \n",
        "          and not token.is_punct]\n",
        "\n",
        "    clean_container.append(\" \".join(txt))\n",
        "    pbar.update(1)\n",
        "  \n",
        "  return clean_container"
      ],
      "metadata": {
        "id": "iRMuX3HZH_xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data4['text_clean'] = text_prepro(data4['text'])"
      ],
      "metadata": {
        "id": "m6ifgcDQH_xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess texts\n",
        "tokens = []\n",
        "\n",
        "for summary in nlp.pipe(data4['text_clean'], disable=[\"ner\"]):\n",
        "  proj_tok = [token.lemma_.lower() for token in summary \n",
        "              if token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV'] \n",
        "              and not token.is_stop\n",
        "              and not token.is_punct] \n",
        "  tokens.append(proj_tok)"
      ],
      "metadata": {
        "id": "sGsLUAarH_xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data4['tokens'] = tokens"
      ],
      "metadata": {
        "id": "GPe-W-GbH_xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Dictionary from the articles: dictionary\n",
        "dictionary = Dictionary(data4['tokens'])\n",
        "\n",
        "# filter out low-frequency / high-frequency stuff, also limit the vocabulary to max 600 words\n",
        "dictionary.filter_extremes(no_below=4, no_above=0.4, keep_n=600)\n",
        "\n",
        "# construct corpus using this dictionary\n",
        "corpus = [dictionary.doc2bow(doc) for doc in data4['tokens']]"
      ],
      "metadata": {
        "id": "-f6ZjjUcH_xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Visualization"
      ],
      "metadata": {
        "id": "3K-lCFNEH_xW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model = LdaMulticore(corpus, id2word=dictionary, num_topics=13, workers = 4, passes=10)"
      ],
      "metadata": {
        "id": "azRIByOCH_xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)"
      ],
      "metadata": {
        "id": "nbFjMIJVH_xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pyLDAvis.display(lda_display)"
      ],
      "metadata": {
        "id": "9LGiTdcCH_xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Topics 2021"
      ],
      "metadata": {
        "id": "9RrKr6WAEb0L"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fezMshFCEjZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Choosing the columns we need to work with and storing them in the value \"data\". This is for saving the \"df\" dataframe for the prediction model later on.\n",
        "data5 = data5[['Authors', 'Author(s) ID','Title', 'Abstract','Year', 'Source title']]"
      ],
      "metadata": {
        "id": "3dGjht3LIAq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Take a random sample of 500 papers. This i for making the model run faster.\n",
        "data5 = data5.sample(n=500)"
      ],
      "metadata": {
        "id": "7PIezFadIAq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Making a new column called \"text\" from the abstract column\n",
        "data5['text'] = data5['Abstract']"
      ],
      "metadata": {
        "id": "f6SMv5nxIAq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZhyZ8ZKIAq4"
      },
      "source": [
        "#Cleaning the text\n",
        "data5['text_clean'] = data5['text'].map(lambda t: prepro.clean(t))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run progress bar and clean up using spacy but without some heavy parts of the pipeline\n",
        "\n",
        "clean_text = []\n",
        "\n",
        "pbar = tqdm.tqdm(total=len(data5['text_clean']),position=0, leave=True)\n",
        "\n",
        "for text in nlp.pipe(data5['text_clean'], disable=[\"tagger\", \"parser\", \"ner\"]):\n",
        "\n",
        "  txt = [token.lemma_.lower() for token in text \n",
        "         if token.is_alpha \n",
        "         and not token.is_stop \n",
        "         and not token.is_punct]\n",
        "\n",
        "  clean_text.append(\" \".join(txt))\n",
        "\n",
        "  pbar.update(1)"
      ],
      "metadata": {
        "id": "Jt6e270NIAq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write everything into a single function for simplicity later on\n",
        "def text_prepro(texts):\n",
        "  \"\"\"\n",
        "  takes in a pandas series (1 column of a DF)\n",
        "  lowercases, normalizes text\n",
        "  \"\"\"\n",
        "  texts_clean = texts.map(lambda t: prepro.clean(t))\n",
        "\n",
        "  clean_container = []\n",
        "\n",
        "  pbar = tqdm.tqdm(total=len(texts_clean),position=0, leave=True)\n",
        "\n",
        "  for text in nlp.pipe(texts_clean, disable=[\"tagger\", \"parser\", \"ner\"]):\n",
        "\n",
        "    txt = [token.lemma_.lower() for token in text \n",
        "          if token.is_alpha \n",
        "          and not token.is_stop \n",
        "          and not token.is_punct]\n",
        "\n",
        "    clean_container.append(\" \".join(txt))\n",
        "    pbar.update(1)\n",
        "  \n",
        "  return clean_container"
      ],
      "metadata": {
        "id": "Yts4-79AIAq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data5['text_clean'] = text_prepro(data5['text'])"
      ],
      "metadata": {
        "id": "XmAaKqTZIAq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess texts\n",
        "tokens = []\n",
        "\n",
        "for summary in nlp.pipe(data5['text_clean'], disable=[\"ner\"]):\n",
        "  proj_tok = [token.lemma_.lower() for token in summary \n",
        "              if token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV'] \n",
        "              and not token.is_stop\n",
        "              and not token.is_punct] \n",
        "  tokens.append(proj_tok)"
      ],
      "metadata": {
        "id": "KCPU4fGYIAq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data5['tokens'] = tokens"
      ],
      "metadata": {
        "id": "0nzwrrRqIAq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Dictionary from the articles: dictionary\n",
        "dictionary = Dictionary(data5['tokens'])\n",
        "\n",
        "# filter out low-frequency / high-frequency stuff, also limit the vocabulary to max 600 words\n",
        "dictionary.filter_extremes(no_below=4, no_above=0.4, keep_n=600)\n",
        "\n",
        "# construct corpus using this dictionary\n",
        "corpus = [dictionary.doc2bow(doc) for doc in data5['tokens']]"
      ],
      "metadata": {
        "id": "jTP0ZxxUIAq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Visualization"
      ],
      "metadata": {
        "id": "SQKzPQMQIAq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model = LdaMulticore(corpus, id2word=dictionary, num_topics=13, workers = 4, passes=10)"
      ],
      "metadata": {
        "id": "bujk8oClIAq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)"
      ],
      "metadata": {
        "id": "CSRRG0Q0IAq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pyLDAvis.display(lda_display)"
      ],
      "metadata": {
        "id": "nhba0QkBIAq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Label Prediction\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YgpTp3BdhysG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yKPVjt-8iZ7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing preprocessor to celan our text\n",
        "!pip install tweet-preprocessor -q\n",
        "\n",
        "# Installing Gensim and PyLDAvis\n",
        "!pip install -qq -U gensim\n",
        "!pip install -qq pyLDAvis\n",
        "\n",
        "# explainability & UMap\n",
        "!pip install eli5\n",
        "!pip install umap-learn -q"
      ],
      "metadata": {
        "id": "QApl8jmPMrxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The foundational imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tqdm #progress bar\n",
        "import preprocessor as prepro # text prepro\n",
        "import matplotlib.pyplot as plt #For plotting using matplot\n",
        "import seaborn as sns #Seaborn which is for visuals, etc\n",
        "sns.set()\n",
        "\n",
        "#IO import - interface\n",
        "import os\n",
        "os.chdir('..')\n",
        "\n",
        "import spacy #spacy for prepro\n",
        "nlp = spacy.load('en_core_web_sm') #instantiating English module for the spacy\n",
        "\n",
        "# sampling, splitting\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# loading ML libraries\n",
        "from sklearn.pipeline import make_pipeline #pipeline creation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer #transforms text to sparse matrix\n",
        "from sklearn.linear_model import LogisticRegression #Logit model\n",
        "from sklearn.metrics import classification_report #that's self explanatory\n",
        "from sklearn.decomposition import TruncatedSVD #dimensionality reduction\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "import altair as alt #viz\n",
        "\n",
        "#explainability\n",
        "import eli5\n",
        "from eli5.lime import TextExplainer\n",
        "\n",
        "# topic modeling\n",
        "\n",
        "from gensim.corpora.dictionary import Dictionary # Import the dictionary builder\n",
        "from gensim.models import LdaMulticore # we'll use the faster multicore version of LDA\n",
        "\n",
        "# Import pyLDAvis\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "\n",
        "%matplotlib inline\n",
        "pyLDAvis.enable_notebook()"
      ],
      "metadata": {
        "id": "3fCZblP-MrxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# topic modeling\n",
        "\n",
        "from gensim.corpora.dictionary import Dictionary # Import the dictionary builder\n",
        "from gensim.models import LdaMulticore # we'll use the faster multicore version of LDA"
      ],
      "metadata": {
        "id": "zwOfCFyGZKGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepro settings\n",
        "prepro.set_options(prepro.OPT.URL, prepro.OPT.NUMBER, prepro.OPT.RESERVED, prepro.OPT.MENTION, prepro.OPT.SMILEY)"
      ],
      "metadata": {
        "id": "3ilUgAu3MrxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing dataset"
      ],
      "metadata": {
        "id": "1JTQ-Un8MrxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = pd.read_csv('/content/M2-NLP-Network-Analysis/Supply chain network - 2017 csv file.csv')\n",
        "data2 = pd.read_csv('/content/M2-NLP-Network-Analysis/Supply chain network - 2018 csv file.csv')\n",
        "data3 = pd.read_csv('/content/M2-NLP-Network-Analysis/Supply chain network - 2019 csv file.csv')\n",
        "data4 = pd.read_csv('/content/M2-NLP-Network-Analysis/Supply chain network - 2020 csv file.csv')\n",
        "data5 = pd.read_csv('/content/M2-NLP-Network-Analysis/Supply chain network - 2021 csv file.csv')\n"
      ],
      "metadata": {
        "id": "wQ7u3oJ7MrxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames = [data1, data2, data3, data4, data5] #creating frame for all datasets"
      ],
      "metadata": {
        "id": "6U6ptczrMrxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat(frames) #Concat all datasets to \"df\""
      ],
      "metadata": {
        "id": "9Jvd590CMrxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##EDA section"
      ],
      "metadata": {
        "id": "Jpl8JAgW-iAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape #Checking the shape of our dataaset, it looks okey-dokey doctor jones"
      ],
      "metadata": {
        "id": "e2G109lcpxNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns #Printing the variables in the datasheet"
      ],
      "metadata": {
        "id": "9NocWfiYMrxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Choosing the columns we need to work with and storing them in the value \"data\". This is for saving the \"df\" dataframe for the prediction model later on.\n",
        "#We select Author(s) ID, Author Keywords and the Abstract variables as we want to work with them. Please note that the Author(s) ID is not used.\n",
        "data = df[['Author(s) ID','Author Keywords', 'Abstract']]"
      ],
      "metadata": {
        "id": "KdrgIAX3MrxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape #Checking the shape of our reassigned dataframe, it is correct as we got two variables."
      ],
      "metadata": {
        "id": "68dFzOxPB8lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head() #Doing a quick head command to check the values on a snippet of the overall set"
      ],
      "metadata": {
        "id": "GOX2b-mxCBOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We are going to drop all the values in the variables without a assigned value, in other words (N/As)\n",
        "data.dropna(inplace=True)\n",
        "data.info()"
      ],
      "metadata": {
        "id": "AWGbqGgC21pZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For this assignment we are going to take a sample of our overall dataframe, and setting it as 2000. \n",
        "#This also helps when running CPU/Ram intensive programs as the lower sample size runs faster.\n",
        "sample_list = data.sample(n=2000)"
      ],
      "metadata": {
        "id": "PgZl0FHf6ZEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We are going to set the variable \"Author Keywords\" as a string, then we are going to use the prepro to clean the variable itself and removing any hashtags, etc.\n",
        "sample_list['Author Keywords']= sample_list['Author Keywords'].astype(str)\n",
        "sample_list['Author Keywords']= sample_list['Author Keywords'].map(lambda t: prepro.clean(t))\n",
        "sample_list['Author Keywords']= sample_list['Author Keywords'].str.replace('#','')"
      ],
      "metadata": {
        "id": "57ySJrTiIcoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepro settings\n",
        "prepro.set_options(prepro.OPT.URL, prepro.OPT.NUMBER, prepro.OPT.RESERVED, prepro.OPT.MENTION, prepro.OPT.SMILEY)"
      ],
      "metadata": {
        "id": "T-YLxwwGOsx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing the sample list to check out if everything works, and it does as you can see the keywords are cleaned up.\n",
        "sample_list.head()"
      ],
      "metadata": {
        "id": "TvGOnqzbR-Nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We want to validate the keywords are correctly processed by the prepro lib, so we create a new addition to our sample_list\n",
        "#The new variable, \"keywords_processed\" have some additional elements to be removed, it wil also be lowercased for all letters in the lines.\n",
        "\n",
        "# Load the regular expression library\n",
        "import re\n",
        "# Remove punctuation\n",
        "sample_list['keywords_processed'] = \\\n",
        "sample_list['Author Keywords'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
        "# Convert the titles to lowercase\n",
        "sample_list['keywords_processed'] = \\\n",
        "sample_list['keywords_processed'].map(lambda x: x.lower())\n",
        "# Print out the first rows of papers\n",
        "sample_list['keywords_processed'].head()"
      ],
      "metadata": {
        "id": "pUORH-LRSa7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_list"
      ],
      "metadata": {
        "id": "Sjx0G4KzS5F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling"
      ],
      "metadata": {
        "id": "voV_n7XZ-qz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will now assign tokens to the keywords we just ran in the preprosition lib, and these tokens are used when we are going to vectorize, and classify them in a dictionary.\n",
        "#The follow commands will have some functions disabled to spare computer processing power, as seen with ner being disabled.\n",
        "#The words will be categorized into nouns, pronouns, adjectives, and adverbiums.\n",
        "\n",
        "tokens = []\n",
        "\n",
        "for summary in nlp.pipe(sample_list['keywords_processed'], disable=[\"ner\"]):\n",
        "  proj_tok = [token.lemma_.lower() for token in summary \n",
        "              if token.pos_ in ['NOUN', 'PROPN', 'ADJ', 'ADV'] \n",
        "              and not token.is_stop\n",
        "              and not token.is_punct] \n",
        "  tokens.append(proj_tok)"
      ],
      "metadata": {
        "id": "vkBfDHnuWuT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We now run the tokens command on the sample list.\n",
        "sample_list['tokens'] = tokens"
      ],
      "metadata": {
        "id": "hiLA9qWQW5KR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Vectorizer is used to covert data over to a matrix, and as we want to look at labeling using author keywords we will assign the function to do likewise.\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "vectors = vectorizer.fit_transform(sample_list['Author Keywords'])"
      ],
      "metadata": {
        "id": "91nPamSbdzer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Dictionary from the articles: dictionary\n",
        "dictionary2 = Dictionary(sample_list['tokens'])\n",
        "# filter out low-frequency / high-frequency stuff, also limit the vocabulary to max 1000 words\n",
        "dictionary2.filter_extremes(no_below=5, no_above=0.5, keep_n=1000)"
      ],
      "metadata": {
        "id": "Xja7dqlFeH3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# construct corpus using this dictionary\n",
        "corpus_tfidf = [dictionary2.doc2bow(doc) for doc in sample_list['tokens']]"
      ],
      "metadata": {
        "id": "YitRS1uae9jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The following function is used for topic modelling, whereas we will use it for modelling through the author keywords. \n",
        "#This function is known as latent dirichlet allocation, LDA, that is aprobabilistic model that assumes each topic is a mixture over an underlying set of words.\n",
        "#These words are the ones we find in author keywords.\n",
        "lda_model = LdaMulticore(corpus_tfidf, id2word=dictionary2, num_topics=3, workers = 4, passes=10)"
      ],
      "metadata": {
        "id": "8urelIPfVWBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We now will display the generated model using LDA by selecting the corpus, dictionary, and the processed LDA model.\n",
        "lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus_tfidf, dictionary2)"
      ],
      "metadata": {
        "id": "MOCXZKpuXf-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#And voila. We selected 3 topics upon the recommendation of the professors for this project, and we can see the following clusters below.\n",
        "pyLDAvis.display(lda_display)"
      ],
      "metadata": {
        "id": "cWm39DR-Vu_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To ensure we got correctly assigned rokens we will now run a quick check.\n",
        "TokenCheck = sample_list['tokens']"
      ],
      "metadata": {
        "id": "vOZjhvloWT_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We got a sample size of 2000, and it seems to match the author keywords that we have used. Check please.\n",
        "TokenCheck"
      ],
      "metadata": {
        "id": "PKx6KsKEXsnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Since we have the LDA create clusters/topics based on the keywords processed we will now assign it to all the abstracts based on the similarity to the keywords they contain\n",
        "transf_corpus = lda_model.get_document_topics(corpus_tfidf)"
      ],
      "metadata": {
        "id": "9y5GOQyOgOyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#But we will first of all produce a list of the produced labels, so we are going to create the following command to apply it for all items.\n",
        "l=[lda_model.get_document_topics(item) for item in corpus_tfidf]"
      ],
      "metadata": {
        "id": "z6aKnxrYgR4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#And this is how the labels look like.\n",
        "l"
      ],
      "metadata": {
        "id": "vvwHiPZxgVFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#So we need to create a applier for the labels ointo the dataframe, this is done using lambda. \n",
        "#In short, it is used for functions that is without a name\n",
        "sorted([('abc', 121),('abc', 231),('abc', 148), ('abc',221)],\n",
        "\n",
        "       key=lambda x: x[1], reverse=True)"
      ],
      "metadata": {
        "id": "4DskHSXFgY7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(l[1], key=lambda x: x[1], reverse=True)[0][0]"
      ],
      "metadata": {
        "id": "sypdHojTga75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now are are going to create the specific command function that we will apply onto the dataframe so we can get it sorted into labels for the keywords.\n",
        "#Also appending it, which means we can add items to the label list.\n",
        "labels = []\n",
        "\n",
        "\n",
        "\n",
        "for blah in l:\n",
        "\n",
        "  ll = sorted(blah, key=lambda x: x[1], reverse=True)[0][0]\n",
        "\n",
        "  labels.append(ll)"
      ],
      "metadata": {
        "id": "ZlJqBNnMge92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = pd.DataFrame(labels)"
      ],
      "metadata": {
        "id": "bkM5R7NsgiRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now are haved applied it, and do a quick test if the labeling works. It seems to do, as we can see some are applied to label group 0, and some to label group 2\n",
        "labels.head()"
      ],
      "metadata": {
        "id": "Fg8rkjRMgkrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We are adding the list of labels to the overall sample list\n",
        "sample_list['labels'] = labels"
      ],
      "metadata": {
        "id": "WIx5ugpZgqqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#And this is how it looks, the labels are now at the far-right of the sample list, which has catagorized the different abstracts based on the author keywords.\n",
        "sample_list"
      ],
      "metadata": {
        "id": "de8LX6t8gt61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we are going to see how many entries that are in the three different label groups, and it seems to be a somewhat even spread.\n",
        "#764 papers belongs to label group 2, 668 to label group 1, and 568 to label group 0.\n",
        "sample_list['labels'].value_counts()"
      ],
      "metadata": {
        "id": "q9Fiy_-khD9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SML part"
      ],
      "metadata": {
        "id": "ATUrb7D6jU7H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have established the labels for the different entries in our sample_size list, we wish to see if we can do some supervised machine learning to make models that can predict based on the dataset"
      ],
      "metadata": {
        "id": "SWq17yHLA0LK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We are now going to test it out by printing a abstract with the label attached it to, the following result is seen below.\n",
        "print(df.iloc[-1]['Abstract'])\n",
        "print(sample_list.iloc[-1]['labels'])"
      ],
      "metadata": {
        "id": "mpzZg5G3jWRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We are making a new text_prepro function which we will run on the supervised machine learning section.\n",
        "# To make sure we are doing the right thing, we decided to make a new function for the intended purposes in the ML\n",
        "def text_prepro(texts):\n",
        "  \"\"\"\n",
        "  takes in a pandas series (1 column of a DF)\n",
        "  lowercases, normalizes the text in the series\n",
        "  \"\"\"\n",
        "  texts_clean = texts.map(lambda t: prepro.clean(t))\n",
        "  texts_clean = texts_clean.str.replace('#','')\n",
        "\n",
        "  clean_container = []\n",
        "\n",
        "  pbar = tqdm.tqdm(total=len(texts_clean),position=0, leave=True)\n",
        "\n",
        "  for text in nlp.pipe(texts_clean, disable=[\"tagger\", \"parser\", \"ner\"]):\n",
        "\n",
        "    txt = [token.lemma_.lower() for token in text \n",
        "          if token.is_alpha \n",
        "          and not token.is_stop \n",
        "          and not token.is_punct]\n",
        "\n",
        "    clean_container.append(\" \".join(txt))\n",
        "    pbar.update(1)\n",
        "  \n",
        "  return clean_container"
      ],
      "metadata": {
        "id": "HOIiNvVykIe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We reset the index of value counts of the label to see if everything is the same as usual, and the label amounts are indeed the same as previously shown.\n",
        "sample_list.labels.value_counts().reset_index()"
      ],
      "metadata": {
        "id": "rTwUOsOWlrns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To ensure there will be no imbalance in the dataset we are going to run a undersampler to ensure the dataset is balanced. \n",
        "# The function is seen as RandomUnderSampler, attached to teh sample_list\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "data_df_res, y_res = rus.fit_resample(sample_list, sample_list['labels'])"
      ],
      "metadata": {
        "id": "kJRwp6_LlynS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into the Training set and Test set (since we have a new output variable)\n",
        "# The selected variables is keywords preprocessed, and the labels we constructed earlier.\n",
        "X_train, X_test, y_train, y_test = train_test_split(sample_list['keywords_processed'], sample_list['labels'], test_size = 0.4, stratify=sample_list['labels'], random_state = 42)"
      ],
      "metadata": {
        "id": "QZAZ6-V2l6Y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#instantiate models and \"bundle up as pipeline\"\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "cls = LogisticRegression()\n",
        "\n",
        "pipe = make_pipeline(tfidf, cls)"
      ],
      "metadata": {
        "id": "hZl3ys9WFKTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.fit(X_train,y_train) # fit model"
      ],
      "metadata": {
        "id": "okQT5dM3E1C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model performance on training set\n",
        "\n",
        "y_eval = pipe.predict(X_train)\n",
        "report = classification_report(y_train, y_eval)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "6ne8wlHfHfyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aforementioned model is the one we have used to \"train\" the model, it seems to perform at a adqeuate performance since the values are around 80-95% accuracy"
      ],
      "metadata": {
        "id": "HRnS6xYKB2V8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# overall weights for the label groups, positive and negative weights seen.\n",
        "eli5.show_weights(pipe, top=20, target_names=[0, 1, 2])"
      ],
      "metadata": {
        "id": "FSnVKv5dHfiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = pipe.predict(X_test)\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "io1aNE98mUsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see the test set is not as accurate as perhaps warranted, however nonetheless it was around 30% accurate. "
      ],
      "metadata": {
        "id": "GANfmfUaCIOg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix assignment and distance"
      ],
      "metadata": {
        "id": "0LjCs7ikns0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now attempt to link up the labels we have created that was made through the author keywords, and link them up to the Abstracts of the papers. In other words, the labels will be categorized onto the abstracts based on the similarity of the words inside the abstracts. \n",
        "\n",
        "To do this we will first assign a ID to the abstracts, and the processed keywords of author keywords, and then assign them accordingly to the sample_size list. This will then allow us to search based on the assigned IDs for the similarity between the two, allowing to see the adjacent topics based on distance."
      ],
      "metadata": {
        "id": "Xv75n7iGCWFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " from sklearn.preprocessing import LabelEncoder\n",
        " import scipy.sparse as ss"
      ],
      "metadata": {
        "id": "yf-wPH-tXys_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We are going to use labelencoder to assign a label to abstract and keywords for making IDs to catagorization purposes.\n",
        "#The following two lines below this one that shows that.\n",
        "le_abstract = LabelEncoder()\n",
        "le_keywords = LabelEncoder()"
      ],
      "metadata": {
        "id": "Lh4dwIYpnu63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_list['Abstract_id'] = le_abstract.fit_transform(sample_list['Abstract'])"
      ],
      "metadata": {
        "id": "vmrPSAFsoBqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_list['keywords_processed_id'] = le_keywords.fit_transform(sample_list['keywords_processed'])"
      ],
      "metadata": {
        "id": "98DiQ2D0oNsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This is a pandas function that returns a new array of given shape and type, filled with ones. We will need this for the matrix.\n",
        "ones = np.ones(len(sample_list), np.uint32)"
      ],
      "metadata": {
        "id": "acEAfVogoH7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The matrix is given the parameters of the abstract ID, and the keywords ID.\n",
        "matrix = ss.coo_matrix((ones, (sample_list['Abstract_id'], sample_list['keywords_processed_id'])))"
      ],
      "metadata": {
        "id": "4KEE7Hkqohk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#And now we will dense it so we can display it.\n",
        "matrix.todense()"
      ],
      "metadata": {
        "id": "WuiQFCNpop87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We are now printing the sample_list so we can see the addition of the assigned IDs, which can be seen at the far_right. \n",
        "sample_list"
      ],
      "metadata": {
        "id": "hBn3-Rkeouoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To validate this, we are going to print out the matrix when the value of 1 is equals to 1. As seen when abstract ID is equal to 1, it returns the abstract ID of 231.\n",
        "# Next line will show that in a more elaborate fashion.\n",
        "np.where(matrix.todense()[1] == 1)"
      ],
      "metadata": {
        "id": "Kqy4FlgdtiJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#As seen here, when the abstract ID is the same as one, it is equal to the keywords preprocessed ID of 231. This means they have the biggest similarity.\n",
        "sample_list[sample_list['Abstract_id'] == 1]"
      ],
      "metadata": {
        "id": "8crNMRg1tlE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Same is done in reversed order, and the result is the same.\n",
        "sample_list[sample_list['keywords_processed_id'] == 231]"
      ],
      "metadata": {
        "id": "qG-yTzRKwkbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We are now going to implemented Truncated, this is a dimensionality reduction method using truncated SVD (aka LSA).\n",
        "from sklearn.decomposition import TruncatedSVD"
      ],
      "metadata": {
        "id": "upqIoxHXxNoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We are setting up a command function with the parameters of three components (labels), and the random state as always being 42.\n",
        "svd = TruncatedSVD(n_components=3, n_iter=7, random_state=42)"
      ],
      "metadata": {
        "id": "xbTti3eCxj1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We are now going to setup matrixes for the abstract ID and keywords ID. It will look the following way:\n",
        "matrix_keywords = svd.fit_transform(matrix)"
      ],
      "metadata": {
        "id": "4nTu5i4PxmfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_abstract = svd.fit_transform(matrix.T)"
      ],
      "metadata": {
        "id": "h24Q1_5RxtLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To see if it works, we will now print matrix_keywords to check if it is working accordingly.\n",
        "matrix_keywords"
      ],
      "metadata": {
        "id": "5CbvgwRExxh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#As we got the sample size being 2000, it does indeed look like it functions as intended.\n",
        "matrix"
      ],
      "metadata": {
        "id": "R7Hm0SUsyWPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Another way to also find the distances between the matrix variables is using cosine distances, we will also do the same.\n",
        "from sklearn.metrics.pairwise import cosine_distances"
      ],
      "metadata": {
        "id": "83vemjc5xy_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_distance_matrix_keywords = cosine_distances(matrix_keywords)"
      ],
      "metadata": {
        "id": "J9kQUXGqyNiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Again, it seems to match accordingly to the sample size that we are running in the project.\n",
        "cosine_distance_matrix_keywords.shape"
      ],
      "metadata": {
        "id": "lOSIA7cpyUKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This function will look up the similarity of the abstracts to the keywords in the the previous matrix.\n",
        "#The way it is setup is seen as below.\n",
        "def similar_abstract(abstract, n):\n",
        "  ix = le_keywords.transform([abstract])[0]\n",
        "  sim_abstract = le_abstract.inverse_transform(np.argsort(cosine_distance_matrix_keywords[ix,:])[:n])\n",
        "  return sim_abstract"
      ],
      "metadata": {
        "id": "uVTqJJOjERVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#If we look at the keywords starting at 858, we can see the following similarties to other keywords being 388, 1941, 1384, and 1658.\n",
        "np.argsort(cosine_distance_matrix_keywords[858,:])[:5]"
      ],
      "metadata": {
        "id": "5Sd-jIFUyrwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we are printing out 5 different abstracts that have this similarity to the keywords, it is abit heavy but nonetheless shows the following result.\n",
        "le_abstract.inverse_transform(np.argsort(cosine_distance_matrix_keywords[858,:])[:5])"
      ],
      "metadata": {
        "id": "11wCxVIvy1MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The final test is checking what abstract ID is similar when we set the keywords ID in the matrix to 7, in this case it would be abstract ID 30.\n",
        "sample_list[sample_list.keywords_processed_id == 7]"
      ],
      "metadata": {
        "id": "TE1o7iWizvNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network analysis\n",
        "\n",
        "---\n",
        "\n",
        "In this section you'll see our network analysis.  This section contains the:\n",
        "\n",
        "*   bipartite network of author - paper network and a adjency matrix hereoff.\n",
        "*   Centrality and community network regarding universities - paper to try and illustrate which universities has the highest output of papers.\n",
        " \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Narcn5VPyXeD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bipartite network ▶ Author - Paper"
      ],
      "metadata": {
        "id": "-X-b4WIBzb_x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OhTBZEgSCmyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "Lo-Ul2Okqh6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pP1WWNjVqTqS"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Only-Mike/M2-NLP-Network-Analysis.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing datasets"
      ],
      "metadata": {
        "id": "AxGd5ATeCcRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = pd.read_csv('/content/M2-NLP-Network-Analysis/Supply chain network - 2017 csv file.csv')\n",
        "data2 = pd.read_csv('/content/M2-NLP-Network-Analysis/Supply chain network - 2018 csv file.csv')\n",
        "data3 = pd.read_csv('/content/M2-NLP-Network-Analysis/Supply chain network - 2019 csv file.csv')\n",
        "data4 = pd.read_csv('/content/M2-NLP-Network-Analysis/Supply chain network - 2020 csv file.csv')\n",
        "data5 = pd.read_csv('/content/M2-NLP-Network-Analysis/Supply chain network - 2021 csv file.csv')\n"
      ],
      "metadata": {
        "id": "HU4sRBP9CcRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames = [data1, data2, data3, data4, data5] #creating frame for all datasets"
      ],
      "metadata": {
        "id": "elkGmO4MCcRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat(frames) #Concat all datasets to \"df\""
      ],
      "metadata": {
        "id": "M60vnY36CcRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reduce the sample to 250 papers\n",
        "df = df.sample(n = 250)"
      ],
      "metadata": {
        "id": "ovE3DwF0E1jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "ynmCkZGr9wuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(index=df[df['Authors'] == '[No author name available]'].index, axis=0)"
      ],
      "metadata": {
        "id": "vxOFhJlY_v53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "kfxHFT_jCcRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "DxiMotVMCcRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make an edgelist\n",
        "edgelist = []\n",
        "for i in df.iterrows():\n",
        "  targets = i[1]['Authors'].split(',')\n",
        "  edgelist_i = [(i[1]['EID'], j) for j in targets]\n",
        "  edgelist.extend(edgelist_i)"
      ],
      "metadata": {
        "id": "34mb-vqLq8mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edgelist[3]  #<---- Show small sample of the data"
      ],
      "metadata": {
        "id": "LJgoqMaphp8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(edgelist)"
      ],
      "metadata": {
        "id": "4k1gTX-x_ITL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx"
      ],
      "metadata": {
        "id": "hJCIFCVbuFUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from networkx.algorithms import bipartite"
      ],
      "metadata": {
        "id": "Wt3Yl1W35u2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brmd71yGT-2p"
      },
      "source": [
        "c0 = set([c[1] for c in edgelist])\n",
        "c1 = set([c[0] for c in edgelist])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmNmKhZmT3WY"
      },
      "source": [
        "B = nx.Graph()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGmNfVXET5f8"
      },
      "source": [
        "# add nodes and edges in their modes\n",
        "B.add_nodes_from(c0, bipartite=0)\n",
        "B.add_nodes_from(c1, bipartite=1)\n",
        "B.add_edges_from(edgelist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_nodes = [n for n in B.nodes if B.nodes[n]['bipartite'] == 0]\n"
      ],
      "metadata": {
        "id": "1_aiuqgl98d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = bipartite.collaboration_weighted_projected_graph(B, top_nodes)"
      ],
      "metadata": {
        "id": "0gNXZ5cKIIAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this code removes nodes with degress (edges) of 1 or less\n",
        "to_be_removed = [x for  x in G.nodes() if G.degree(x) <= 1]\n",
        "G.remove_nodes_from(to_be_removed)"
      ],
      "metadata": {
        "id": "aYYF0eDznInP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edges_df = nx.to_pandas_edgelist(B)\n",
        "edges_df"
      ],
      "metadata": {
        "id": "2u-uQTeEoPvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For visualization\n",
        "!pip install -U bokeh\n",
        "!pip install -q holoviews"
      ],
      "metadata": {
        "id": "l9fnudLOBdBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the libraries and link to the bokeh backend\n",
        "import holoviews as hv\n",
        "from holoviews import opts\n",
        "hv.extension('bokeh')\n",
        "from bokeh.plotting import show\n",
        "\n",
        "# Setting the default figure size a bit larger\n",
        "defaults = dict(width=750, height=750, padding=0.1,\n",
        "                xaxis=None, yaxis=None)\n",
        "hv.opts.defaults(\n",
        "    opts.EdgePaths(**defaults), opts.Graph(**defaults), opts.Nodes(**defaults))"
      ],
      "metadata": {
        "id": "3dJXEDm4BkQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_layout = nx.layout.kamada_kawai_layout(G)"
      ],
      "metadata": {
        "id": "MPpZP6sashFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g_plot = hv.Graph.from_networkx(G, G_layout).opts(tools=['hover'],\n",
        "                                                                        directed=False,\n",
        "                                                                        edge_alpha=0.4,\n",
        "                                                                        node_size= 5,\n",
        "                                                                        #node_color='seniority', cmap='Set1',\n",
        "                                                                        legend_position='right')\n",
        "show(hv.render(g_plot))"
      ],
      "metadata": {
        "id": "lCl0aZXUsTPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Centrality and communty network ▶ University - Paper"
      ],
      "metadata": {
        "id": "S_-10n13zqxx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jO4OQP3UCfeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the data again to run the full dataset"
      ],
      "metadata": {
        "id": "azQ_VtGoOSns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = pd.read_csv('/content/M2-NLP-Network-Analysis/Supply chain network - 2017 csv file.csv')\n",
        "data2 = pd.read_csv('/content/M2-NLP-Network-Analysis/Supply chain network - 2018 csv file.csv')\n",
        "data3 = pd.read_csv('/content/M2-NLP-Network-Analysis/Supply chain network - 2019 csv file.csv')\n",
        "data4 = pd.read_csv('/content/M2-NLP-Network-Analysis/Supply chain network - 2020 csv file.csv')\n",
        "data5 = pd.read_csv('/content/M2-NLP-Network-Analysis/Supply chain network - 2021 csv file.csv')\n"
      ],
      "metadata": {
        "id": "Vc1cMwJoOS11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames = [data1, data2, data3, data4, data5] #creating frame for all datasets"
      ],
      "metadata": {
        "id": "iQgISchMOS11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat(frames) #Concat all datasets to \"df\""
      ],
      "metadata": {
        "id": "bNxxWKOHOS11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(index=df[df['Authors'] == '[No author name available]'].index, axis=0)"
      ],
      "metadata": {
        "id": "NOBmydRDPYUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Affiliations'].value_counts(ascending=False).nlargest(20)"
      ],
      "metadata": {
        "id": "-wfQha1jj90h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing the EID column from string to integer.\n",
        "df['EID'] = df['EID'].str.replace(r's', '')\n",
        "df['EID'] = df['EID'].str.replace(r'.', '')\n",
        "df['EID'] = df['EID'].str.replace(r'-', '')\n",
        "df['EID'].astype(int)"
      ],
      "metadata": {
        "id": "QLodou8va9c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['EID']"
      ],
      "metadata": {
        "id": "f49CPVVOa_AP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Edgelist construction"
      ],
      "metadata": {
        "id": "wBOQezn5gGfK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp8p1q2gss_5"
      },
      "source": [
        "# select Authors and EID\n",
        "data_select = [['Authors', 'EID', 'Affiliations']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znWq2naJsmjB"
      },
      "source": [
        "# create edge Dataframe by merging it with itself.\n",
        "edges = pd.merge(data_select, data_select, on='Affiliations')\n",
        "edges.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBF4Mq-Qu8m_"
      },
      "source": [
        "# Filter out the self-edges\n",
        "edges = edges[edges['EID_x'] != edges['EID_y']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "517JQi3Z9q-y"
      },
      "source": [
        "# grouping to aggregate multiple co-occurences and to generate a weight: \n",
        "edges = edges.groupby(['EID_x', 'EID_y']).size().reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edges.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "wJZUu5HtSiya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G71jk67mQKf"
      },
      "source": [
        "# column \"0\" is now our weight\n",
        "edges.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoXr1hBp-Zfg"
      },
      "source": [
        "# Eename the \"0\" column to weight\n",
        "edges.rename({0:'weight'}, axis = 1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edges.head()"
      ],
      "metadata": {
        "id": "15LVEdp-QEhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUEjSKGIuqKg"
      },
      "source": [
        "len(edges)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOriUK9tttJi"
      },
      "source": [
        "# Create network object from pandas edgelist\n",
        "G = nx.from_pandas_edgelist(edges, source='EID_x', target='EID_y', edge_attr='weight', create_using=nx.Graph())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiD__UWZHHhr"
      },
      "source": [
        "# We can create a node-attribute dictionary directly from the dataframe (using pandas to_dict)\n",
        "node_attributes = data_select[['EID','Affiliations']].set_index('EID').drop_duplicates().to_dict('index')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrEHBF0-HQLX"
      },
      "source": [
        "# We now can include the degree as node-attribute\n",
        "nx.set_node_attributes(G, {G.degree(): 'degree'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnH1pdXQHViS"
      },
      "source": [
        "# and use the node_attribute object to include all that in the graph object\n",
        "nx.set_node_attributes(G, node_attributes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2gjulPHuOr0"
      },
      "source": [
        "len(G.nodes())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx8Qvd7huRa_"
      },
      "source": [
        "len(G.edges())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqPiAfUsAzdu"
      },
      "source": [
        "# Subset the graph keeping only nodes with degree > 1\n",
        "G = nx.subgraph(G, [n for n,d in G.degree() if d > 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuMn2VOauT7b"
      },
      "source": [
        "# Here we can calculate different centrality indicators as well as partition (community detection)\n",
        "centrality_dgr = nx.degree_centrality(G)\n",
        "centrality_eig = nx.eigenvector_centrality_numpy(G, weight = 'weight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from community import community_louvain"
      ],
      "metadata": {
        "id": "Yn-9iy7Dg4F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "partition = community_louvain.best_partition(G)"
      ],
      "metadata": {
        "id": "-lDTeZNDD12v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwnlBDhbvh_L"
      },
      "source": [
        "# All these indicators can now be set as attribute of the Graph\n",
        "nx.set_node_attributes(G, centrality_dgr, 'dgr')\n",
        "nx.set_node_attributes(G, centrality_eig, 'eig')\n",
        "nx.set_node_attributes(G, partition, 'partition')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Quick plot of the centrality degree\n",
        "nx.draw_kamada_kawai(G, node_color=list(partition.values()), node_size=[v * 5 for v in dict(G.degree()).values()])"
      ],
      "metadata": {
        "id": "o_m-Oj9rsNOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm1rKpfowzUm"
      },
      "source": [
        "# Turn the Graph object (NetworkX) into a Dataframe\n",
        "nodes_df = pd.DataFrame.from_dict(dict(G.nodes(data=True)), orient='index')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodes_df.head()"
      ],
      "metadata": {
        "id": "fdzVfvrxEzFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcuHdb00w_OK"
      },
      "source": [
        "# Sort dataframe by eigenvector.\n",
        "nodes_df.sort_values('eig', ascending=False)[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRdOF2zzGQrh"
      },
      "source": [
        "# How many communities are there.\n",
        "nodes_df.partition.nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoLGzqodERRw"
      },
      "source": [
        "#define top10_com as partition value counts\n",
        "top10_com = nodes_df.partition.value_counts()[:10].index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9cXstUUEU5v"
      },
      "source": [
        "top10_com_nodes = nodes_df[nodes_df.partition.isin(top10_com)].index\n",
        "\n",
        "# Make a subgraph\n",
        "g_sub = nx.subgraph(G, top10_com_nodes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgyVieZWEYAx"
      },
      "source": [
        "# Now we will limit the resulting dataframe to the top10 communities\n",
        "nodes_df_top10 = nodes_df[nodes_df.partition.isin(top10_com)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmxaTREZEbns"
      },
      "source": [
        "nodes_df_top10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW2vcwhlFYp5"
      },
      "source": [
        "# with the highest eigenvector centrality\n",
        "top_affiliations = nodes_df_top10.groupby('partition')['eig'].nlargest(5).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bh6UUCbCKKEX"
      },
      "source": [
        "top_affiliations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYnqouOlFryg"
      },
      "source": [
        "# After that we need to bring back ID's (rename) and Names (merge)\n",
        "top_affiliations.rename({'level_1':'EID'}, axis=1, inplace=True)\n",
        "top_affiliations = pd.merge(top_affiliations, data_select[['Affiliations','EID']].drop_duplicates(), on='EID', how='inner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeB4L0isFyTh"
      },
      "source": [
        "top_affiliations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a16a8CuDLQn9"
      },
      "source": [
        "### Visualisations\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m9VbUQxF0ql"
      },
      "source": [
        "!pip install -qq holoviews\n",
        "!pip install -qq -U bokeh\n",
        "!pip install -qq datashader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f2pGegVGJYM"
      },
      "source": [
        "# Import the libraries and link to the bokeh backend\n",
        "import holoviews as hv\n",
        "from holoviews import opts\n",
        "hv.extension('bokeh')\n",
        "from bokeh.plotting import show\n",
        "kwargs = dict(width=800, height=800, xaxis=None, yaxis=None)\n",
        "opts.defaults(opts.Nodes(**kwargs), opts.Graph(**kwargs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw0WO-kwGWcA"
      },
      "source": [
        "# keeping only top nodes (extreme subsetting)\n",
        "top_central_nodes = nodes_df[nodes_df.eig > nodes_df.eig.quantile(0.99)].index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4In4-xn5GbaW"
      },
      "source": [
        "# Create subset graph\n",
        "g_sub = nx.subgraph(G, top_central_nodes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the centrality degree\n",
        "cent_degree = dict(nx.degree(G))"
      ],
      "metadata": {
        "id": "5RDEmGPIjAhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the eigen degree\n",
        "cent_eigen = dict(nx.eigenvector_centrality(G))"
      ],
      "metadata": {
        "id": "da8MZFnMyOqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot centrality degree graph\n",
        "nx.set_node_attributes(G, cent_degree, 'cent_degree')\n",
        "\n",
        "g_plot = hv.Graph.from_networkx(G, G_layout).opts(tools=['hover'],\n",
        "                                                  node_size='cent_degree')\n",
        "\n",
        "show(hv.render(g_plot))"
      ],
      "metadata": {
        "id": "GISIqaU4w6SY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot centrality eigenvalue graph\n",
        "nx.set_node_attributes(G, cent_eigen, 'cent_eigen')\n",
        "\n",
        "g_plot = hv.Graph.from_networkx(G, G_layout).opts(tools=['hover'],\n",
        "                                                  node_size='cent_eigen')\n",
        "\n",
        "show(hv.render(g_plot))"
      ],
      "metadata": {
        "id": "yecF76TsxCM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3EhiPkA6Mqe"
      },
      "source": [
        "# Find the optimal partition with the Louvain algorithm.\n",
        "com = community_louvain.best_partition(G)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuySDZyFRl06"
      },
      "source": [
        "# The number of communities detected\n",
        "max(com.values())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxBtmmy6TVzf"
      },
      "source": [
        "#Plot community network\n",
        "nx.set_node_attributes(G, com, 'community')\n",
        "\n",
        "g_plot = hv.Graph.from_networkx(G, G_layout).opts(tools=['hover'],\n",
        "                                                  node_size='cent_degree', \n",
        "                                                  node_color='community', cmap=plt.cm.Set1,\n",
        "                                                  legend_position='right',\n",
        "                                                  edge_alpha=0.25)\n",
        "\n",
        "show(hv.render(g_plot))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}